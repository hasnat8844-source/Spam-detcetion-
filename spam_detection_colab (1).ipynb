{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prediction helper (load saved model & vectorizer)"
      ],
      "metadata": {
        "id": "gzhu7bmBOs7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imports and Setup"
      ],
      "metadata": {
        "id": "g_6kYqAGTYJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# ML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Colab file utilities\n",
        "try:\n",
        "    from google.colab import files\n",
        "except Exception:\n",
        "    files = None\n",
        "\n",
        "# Visualization helper\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "print(\"1) Ensuring NLTK stopwords are available...\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "gDrrg00FfyTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Upload CSV File"
      ],
      "metadata": {
        "id": "afXWYTdlTix4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if files is None:\n",
        "    raise EnvironmentError(\"This cell expects to run in Google Colab (google.colab.files not available).\")\n",
        "\n",
        "print(\"\\n2) Upload your dataset CSV (e.g., spam.csv). Use the upload dialog that appears...\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise FileNotFoundError(\"No file uploaded. Re-run the cell and upload the CSV file.\")\n",
        "\n",
        "# take first uploaded file\n",
        "filename = next(iter(uploaded))\n",
        "print(\"Uploaded file:\", filename)"
      ],
      "metadata": {
        "id": "0oAwaSdOTjIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Read CSV with Encoding Fallback"
      ],
      "metadata": {
        "id": "A5oTWJDzTjla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3) Reading CSV (trying utf-8, latin1, iso-8859-1)...\")\n",
        "encodings = ['utf-8', 'latin1', 'iso-8859-1']\n",
        "df = None\n",
        "for enc in encodings:\n",
        "    try:\n",
        "        df = pd.read_csv(filename, encoding=enc)\n",
        "        print(f\"Read with encoding: {enc}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Failed with {enc}: {type(e).__name__}\")\n",
        "if df is None:\n",
        "    raise ValueError(\"Unable to read CSV with tried encodings. Please check file or rerun with a different file.\")\n",
        "\n",
        "# ------------------ Inspect & normalize columns ------------------\n",
        "print(\"\\n4) Inspecting columns:\", list(df.columns))\n",
        "# Common column names: 'v1','v2' or 'label','text' or 'class','message'\n",
        "col_lower = [c.lower() for c in df.columns]\n",
        "\n",
        "# find text column\n",
        "text_col = None\n",
        "for cand in ['text', 'message', 'body', 'msg', 'v2']:\n",
        "    if cand in col_lower:\n",
        "        text_col = df.columns[col_lower.index(cand)]\n",
        "        break\n",
        "# find label column\n",
        "label_col = None\n",
        "for cand in ['label', 'class', 'v1', 'type']:\n",
        "    if cand in col_lower:\n",
        "        label_col = df.columns[col_lower.index(cand)]\n",
        "        break\n",
        "\n",
        "if text_col is None or label_col is None:\n",
        "    raise KeyError(\"Couldn't detect text and/or label columns automatically. Please ensure CSV has columns like ('v1','v2') or ('label','text'). Columns found: \" + \", \".join(df.columns))\n",
        "\n",
        "# rename for consistency\n",
        "df = df.rename(columns={text_col: 'text', label_col: 'label'})\n",
        "df = df[['text', 'label']].dropna().reset_index(drop=True)\n",
        "\n",
        "print(f\"Using text column: 'text' & label column: 'label' â€” sample rows: {min(3, len(df))}\")\n",
        "print(df.head(3))\n",
        "\n",
        "# Normalize labels to lower-case strings\n",
        "df['label'] = df['label'].astype(str).str.lower().str.strip()\n",
        "# If labels are not ham/spam map: try common maps (1/0, spam/ham etc.)\n",
        "if set(df['label'].unique()) - set(['spam','ham']) and set(df['label'].unique()) & set(['1','0','true','false']):\n",
        "    # try numeric mapping\n",
        "    df['label'] = df['label'].replace({'1': 'spam', '0': 'ham', 'true': 'spam', 'false': 'ham', 't': 'spam', 'f': 'ham'})\n",
        "print(\"Label value counts:\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "amEhu9e3TkMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Text Cleaning"
      ],
      "metadata": {
        "id": "QUmEcBXOTku0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n5) Cleaning text (lowercase, remove non-letters, remove stopwords, stemming)...\")\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    # replace urls, emails, numbers with space\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)  # keep letters and spaces only\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [stemmer.stem(tok) for tok in tokens if tok not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply cleaning with progress if large\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print(\"Example original -> cleaned:\")\n",
        "for i in range(min(2, len(df))):\n",
        "    print(\"ORIG:\", df['text'].iloc[i])\n",
        "    print(\"CLEAN:\", df['clean_text'].iloc[i])\n",
        "    print(\"---\")\n",
        "\n",
        "# Remove rows with empty cleaned text\n",
        "before = len(df)\n",
        "df = df[df['clean_text'].str.strip() != \"\"].reset_index(drop=True)\n",
        "after = len(df)\n",
        "if before != after:\n",
        "    print(f\"Removed {before-after} rows with empty cleaned text.\")"
      ],
      "metadata": {
        "id": "-MPKkHtZTlCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Feature Extraction (TF-IDF)"
      ],
      "metadata": {
        "id": "cmh4L37sTlRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n6) TF-IDF vectorization (max_features=5000)...\")\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df['clean_text']).toarray()\n",
        "# Map labels to binary\n",
        "label_map = {'ham': 0, 'spam': 1}\n",
        "if not set(df['label'].unique()).issubset(set(label_map.keys())):\n",
        "    # attempt to map anything else: take most frequent as ham and other as spam if unclear\n",
        "    uniq = list(df['label'].unique())\n",
        "    print(\"Unrecognized label values detected:\", uniq)\n",
        "    # try numeric mapping if possible\n",
        "    df['label'] = df['label'].apply(lambda x: 'spam' if x in ['1','yes','y','spam'] else ('ham' if x in ['0','no','n','ham'] else x))\n",
        "    # final fallback: map top freq to ham, rest to spam\n",
        "    if not set(df['label'].unique()).issubset(set(label_map.keys())):\n",
        "        most_common = df['label'].value_counts().idxmax()\n",
        "        df['label'] = df['label'].apply(lambda x: 'ham' if x==most_common else 'spam')\n",
        "y = df['label'].map(label_map).values\n",
        "\n",
        "print(\"TF-IDF shape:\", X.shape)\n",
        "print(\"Label distribution (binary):\")\n",
        "print(pd.Series(y).value_counts())"
      ],
      "metadata": {
        "id": "EOCvofAbTlgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Data Splitting"
      ],
      "metadata": {
        "id": "x096up2JTlv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n7) Splitting data (train/test 80/20)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(\"Shapes:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "2D5-mq-ITmBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Model Training"
      ],
      "metadata": {
        "id": "99oPuxJNUYfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n8) Training models: MultinomialNB, SVM (linear), RandomForest\")\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "scores = {}\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        scores[name] = acc\n",
        "        trained_models[name] = model\n",
        "        print(f\"{name} Accuracy: {acc:.4f}\")\n",
        "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(4,3))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham','Spam'], yticklabels=['Ham','Spam'])\n",
        "        plt.title(f\"{name} Confusion Matrix\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {name}: {e}\")"
      ],
      "metadata": {
        "id": "wAVgcjCfUdQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Best Model Selection"
      ],
      "metadata": {
        "id": "2DzXosAsUfc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not scores:\n",
        "    raise RuntimeError(\"No models trained successfully.\")\n",
        "best_name = max(scores, key=scores.get)\n",
        "best_model = trained_models[best_name]\n",
        "print(f\"\\n9) Best model based on accuracy: {best_name} (Accuracy={scores[best_name]:.4f})\")"
      ],
      "metadata": {
        "id": "o59sHg0sUjE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Word Clouds"
      ],
      "metadata": {
        "id": "tZJff9sxUlW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n10) Generating word clouds for spam and ham (may take a moment)...\")\n",
        "try:\n",
        "    spam_text = \" \".join(df[df['label']=='spam']['clean_text'])\n",
        "    ham_text = \" \".join(df[df['label']=='ham']['clean_text'])\n",
        "    def show_wc(text, title):\n",
        "        if not text.strip():\n",
        "            print(\"No text for\", title)\n",
        "            return\n",
        "        wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.imshow(wc, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "    show_wc(spam_text, \"Spam Word Cloud\")\n",
        "    show_wc(ham_text, \"Ham Word Cloud\")\n",
        "except Exception as e:\n",
        "    print(\"Word cloud generation failed:\", e)"
      ],
      "metadata": {
        "id": "tlzY7hOeUpVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Save Model and Vectorizer"
      ],
      "metadata": {
        "id": "cd0KV8DpUra0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = 'best_spam_model.joblib'\n",
        "vec_filename = 'tfidf_vectorizer.joblib'\n",
        "print(f\"\\n11) Saving best model ({best_name}) to {model_filename} and vectorizer to {vec_filename} ...\")\n",
        "joblib.dump(best_model, model_filename)\n",
        "joblib.dump(tfidf, vec_filename)\n",
        "\n",
        "# Download files (Colab)\n",
        "if files is not None:\n",
        "    try:\n",
        "        files.download(model_filename)\n",
        "        files.download(vec_filename)\n",
        "    except Exception as e:\n",
        "        print(\"Automatic download failed (this sometimes happens). Files are saved in the Colab working directory. Use the file browser to download manually. Error:\", e)"
      ],
      "metadata": {
        "id": "p4pni71EUrxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Prediction Function and Testing"
      ],
      "metadata": {
        "id": "ic4VTE63U338"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n12) Defining predict_email(...) function and testing with samples.\")\n",
        "\n",
        "def predict_email(email_text, model_path=model_filename, vec_path=vec_filename):\n",
        "    # load model & vectorizer\n",
        "    m = joblib.load(model_path)\n",
        "    v = joblib.load(vec_path)\n",
        "    cleaned = clean_text(email_text)\n",
        "    vec = v.transform([cleaned]).toarray()\n",
        "    pred = m.predict(vec)[0]\n",
        "    prob = None\n",
        "    try:\n",
        "        if hasattr(m, \"predict_proba\"):\n",
        "            p = m.predict_proba(vec)[0]\n",
        "            prob = {'ham_prob': f\"{p[0]*100:.2f}%\", 'spam_prob': f\"{p[1]*100:.2f}%\"}\n",
        "    except Exception:\n",
        "        prob = None\n",
        "    return {'prediction': 'spam' if pred==1 else 'ham', 'probabilities': prob, 'cleaned_text': cleaned}\n",
        "\n",
        "# Test samples\n",
        "sample_ham = \"Hi there, just checking in to see if you're free for lunch tomorrow?\"\n",
        "sample_spam = \"CONGRATULATIONS!! You've won a $1000 Walmart gift card. Click here to claim!\"\n",
        "\n",
        "print(\"Sample ham ->\", predict_email(sample_ham))\n",
        "print(\"Sample spam ->\", predict_email(sample_spam))\n",
        "\n"
      ],
      "metadata": {
        "id": "mvXUE9pLU4us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}